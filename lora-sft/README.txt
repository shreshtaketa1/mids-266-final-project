This subfolder contains the code for the LoRA SFT work done by Erica Landreth.

There is one folder for each fine-tuned model and/or baseline model:
* b32: bootstrapped rank-32 LoRA model
* hu: HuatuoGPT-o1 baseline model
* l3: Llama 3 8B Instruct baseline model
* l32: rank-32 LoRA model tuning only attention parameters
* r8: rank-8 LoRA model
* r32: rank-32 LoRA model

Within each subfolder, you'll find the following notebooks:
~ All folders ~
* ...-eval: Create model output for high-temperature cases
* ...-eval-ref: Create model output for low-temperature ("reference") case
* ...-score: Score model against ground-truth data
* ...-score-ref: Score model against reference data
~ Subset of folders ~
* (b32 only): create_bootstrap_dataset: used to generate the bootstrap dataset
* (fine-tuned models only): ...-peft-... : LoRA fine-tuning training process

All fine-tuned models are stored on huggingface and are currently private; reach out to Erica Landreth for access: erica.landreth@berkeley.edu

The model_output subfolder contains text generated by each model in response to the test questions.

The analysis_v2 contains the analysis used to generate all tables and figures in the report.